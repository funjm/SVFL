{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363f2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(StudentMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(size, size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "#     权重初始化为全1,bias为0\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.ones_(m.weight.data)\n",
    "                m.bias.data.zero_()\n",
    "                print(\"\\nafter init:  \",m) \n",
    "                print('weight is: ', m.weight)\n",
    "                print(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f60410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  tensor(0.8440, grad_fn=<SumBackward0>)\n",
      "tensor([0.4134, 0.3539])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6442, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0357, -0.3320])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.7216, grad_fn=<SumBackward0>)\n",
      "tensor([-0.2453, -0.3792])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6327, grad_fn=<SumBackward0>)\n",
      "tensor([-0.2321, -0.1480])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5968, grad_fn=<SumBackward0>)\n",
      "tensor([-0.1180,  0.1412])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6413, grad_fn=<SumBackward0>)\n",
      "tensor([0.0056, 0.3215])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6418, grad_fn=<SumBackward0>)\n",
      "tensor([0.0852, 0.3263])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5922, grad_fn=<SumBackward0>)\n",
      "tensor([0.1062, 0.2015])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5590, grad_fn=<SumBackward0>)\n",
      "tensor([0.0799, 0.0141])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5779, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0315, -0.1639])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6119, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0112, -0.2665])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.6108, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0310, -0.2671])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5782, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0257, -0.1826])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5520, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0043, -0.0503])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5559, grad_fn=<SumBackward0>)\n",
      "tensor([0.0188, 0.0847])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5771, grad_fn=<SumBackward0>)\n",
      "tensor([0.0297, 0.1790])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5863, grad_fn=<SumBackward0>)\n",
      "tensor([0.0214, 0.2064])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5751, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0030,  0.1674])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5589, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0325,  0.0832])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5539, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0533, -0.0149])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5598, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0550, -0.0951])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5653, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0344, -0.1341])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5640, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0019, -0.1255])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5596, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0408, -0.0797])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5574, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0671, -0.0173])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5571, grad_fn=<SumBackward0>)\n",
      "tensor([0.0701, 0.0388])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5561, grad_fn=<SumBackward0>)\n",
      "tensor([0.0480, 0.0713])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5545, grad_fn=<SumBackward0>)\n",
      "tensor([0.0085, 0.0738])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5548, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0344,  0.0516])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5569, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0646,  0.0179])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5571, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0711, -0.0124])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5539, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0523, -0.0292])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5507, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0156, -0.0297])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5513, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0249, -0.0185])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5543, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0541, -0.0044])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5554, grad_fn=<SumBackward0>)\n",
      "tensor([0.0617, 0.0046])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5529, grad_fn=<SumBackward0>)\n",
      "tensor([0.0464, 0.0050])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5502, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0156, -0.0011])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5503, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0180, -0.0078])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5523, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0415, -0.0092])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5531, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0470, -0.0028])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5518, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0342,  0.0090])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5504, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0103,  0.0200])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5504, grad_fn=<SumBackward0>)\n",
      "tensor([0.0141, 0.0234])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5511, grad_fn=<SumBackward0>)\n",
      "tensor([0.0293, 0.0160])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5512, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0308, -0.0001])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5509, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0201, -0.0181])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5507, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0036, -0.0295])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5506, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0108, -0.0285])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5503, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0176, -0.0148])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5503, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0157,  0.0057])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5505, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0078,  0.0238])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5507, grad_fn=<SumBackward0>)\n",
      "tensor([0.0009, 0.0312])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5503, grad_fn=<SumBackward0>)\n",
      "tensor([0.0062, 0.0247])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0067, 0.0077])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5500, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0037, -0.0119])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5504, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0003, -0.0250])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5504, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0012, -0.0261])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5500, grad_fn=<SumBackward0>)\n",
      "tensor([-3.0115e-05, -1.5425e-02])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0021, 0.0011])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5500, grad_fn=<SumBackward0>)\n",
      "tensor([0.0031, 0.0153])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5502, grad_fn=<SumBackward0>)\n",
      "tensor([0.0014, 0.0209])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5501, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0023,  0.0164])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0059,  0.0051])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0069, -0.0068])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5500, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0040, -0.0135])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5500, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0015, -0.0127])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0068, -0.0063])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0089, 0.0017])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0064, 0.0069])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0005, 0.0074])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0057,  0.0042])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-8.8583e-03,  6.9976e-05])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0074, -0.0025])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0022, -0.0026])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0038, -0.0011])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0075, 0.0003])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([0.0069, 0.0004])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0029, -0.0008])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0022, -0.0018])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0055, -0.0015])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5499, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0055,  0.0003])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0027,  0.0027])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0010, 0.0038])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0035, 0.0026])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0037, -0.0004])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0019, -0.0034])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0005, -0.0046])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0019, -0.0031])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0019,  0.0002])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0009,  0.0034])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0002, 0.0046])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0007, 0.0030])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0004, -0.0002])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-3.5062e-05, -3.0071e-03])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([-0.0001, -0.0039])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([ 0.0002, -0.0025])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0006, 0.0002])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([0.0006, 0.0024])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor(0.5498, grad_fn=<SumBackward0>)\n",
      "tensor([4.3854e-05, 2.8869e-03])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "featureSet_list = [1, 2]\n",
    "label = torch.tensor([0., 1], requires_grad=True)\n",
    "\n",
    "\n",
    "hidden_layers = list()\n",
    "transferBridge_set = list()\n",
    "\n",
    "for i in range(3):\n",
    "    transferBridge_set.append(StudentMLP(2))\n",
    "    hidden_layers.append(label)\n",
    "\n",
    "    \n",
    "test_common_representation = torch.tensor([0., 1], requires_grad=True)\n",
    "test_optimizer = optim.Adam([test_common_representation], lr=1, betas=(0.9, 0.999))\n",
    "\n",
    "transferBridge_criterion = nn.MSELoss()\n",
    "\n",
    "for converge in range(100):\n",
    "    tmp_loss = torch.zeros((len(featureSet_list)), dtype=torch.float32)\n",
    "    for i in range(len(featureSet_list)):\n",
    "        outputs = transferBridge_set[i](test_common_representation)\n",
    "        tmp_loss[i] = transferBridge_criterion(hidden_layers[i], outputs)\n",
    "\n",
    "    test_loss = torch.sum(tmp_loss)\n",
    "    print(\"test loss: \", test_loss)\n",
    "\n",
    "    test_loss.backward()\n",
    "    test_optimizer.step()\n",
    "    print(test_common_representation.grad)\n",
    "    test_optimizer.zero_grad()\n",
    "    print(test_common_representation.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51318a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  tensor([0.5663, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<CopySlices>)\n",
      "tensor([-0.3490, -0.2414])\n",
      "tensor([0., 0.])\n",
      "test loss:  tensor([0.5663, 0.2667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-77a5b2b06266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconverge\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtest_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_common_representation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FL/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FL/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "featureSet_list = [1, 2]\n",
    "label = torch.tensor([0., 1], requires_grad=True)\n",
    "\n",
    "\n",
    "hidden_layers = list()\n",
    "transferBridge_set = list()\n",
    "\n",
    "for i in range(3):\n",
    "    transferBridge_set.append(StudentMLP(2))\n",
    "    hidden_layers.append(label)\n",
    "\n",
    "    \n",
    "test_common_representation = torch.tensor([0., 1], requires_grad=True)\n",
    "test_optimizer = optim.Adam([test_common_representation], lr=1, betas=(0.9, 0.999))\n",
    "\n",
    "transferBridge_criterion = nn.MSELoss()\n",
    "\n",
    "test_loss = torch.zeros(100, dtype=torch.float32)\n",
    "for converge in range(100):\n",
    "    tmp_loss = torch.zeros((len(featureSet_list)), dtype=torch.float32)\n",
    "    for i in range(len(featureSet_list)):\n",
    "        outputs = transferBridge_set[i](test_common_representation)\n",
    "        tmp_loss[i] = transferBridge_criterion(hidden_layers[i], outputs)\n",
    "\n",
    "    test_loss[converge] = torch.sum(tmp_loss)\n",
    "    print(\"test loss: \", test_loss)\n",
    "\n",
    "    test_loss[converge].backward()\n",
    "    test_optimizer.step()\n",
    "    print(test_common_representation.grad)\n",
    "    test_optimizer.zero_grad()\n",
    "    print(test_common_representation.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6463ad41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  tensor(3.6152, grad_fn=<SumBackward0>)\n",
      "tensor(3.6152, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.9550, grad_fn=<SumBackward0>)\n",
      "tensor(0.9550, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.8780, grad_fn=<SumBackward0>)\n",
      "tensor(0.8780, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(1.5786, grad_fn=<SumBackward0>)\n",
      "tensor(1.5786, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(1.6144, grad_fn=<SumBackward0>)\n",
      "tensor(1.6144, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(1.1092, grad_fn=<SumBackward0>)\n",
      "tensor(1.1092, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.5799, grad_fn=<SumBackward0>)\n",
      "tensor(0.5799, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.5259, grad_fn=<SumBackward0>)\n",
      "tensor(0.5259, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.8011, grad_fn=<SumBackward0>)\n",
      "tensor(0.8011, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.9335, grad_fn=<SumBackward0>)\n",
      "tensor(0.9335, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.8460, grad_fn=<SumBackward0>)\n",
      "tensor(0.8460, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.6434, grad_fn=<SumBackward0>)\n",
      "tensor(0.6434, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4814, grad_fn=<SumBackward0>)\n",
      "tensor(0.4814, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4524, grad_fn=<SumBackward0>)\n",
      "tensor(0.4524, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.5369, grad_fn=<SumBackward0>)\n",
      "tensor(0.5369, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.6363, grad_fn=<SumBackward0>)\n",
      "tensor(0.6363, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.6593, grad_fn=<SumBackward0>)\n",
      "tensor(0.6593, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.5876, grad_fn=<SumBackward0>)\n",
      "tensor(0.5876, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4737, grad_fn=<SumBackward0>)\n",
      "tensor(0.4737, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3914, grad_fn=<SumBackward0>)\n",
      "tensor(0.3914, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3819, grad_fn=<SumBackward0>)\n",
      "tensor(0.3819, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4308, grad_fn=<SumBackward0>)\n",
      "tensor(0.4308, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4855, grad_fn=<SumBackward0>)\n",
      "tensor(0.4855, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.5002, grad_fn=<SumBackward0>)\n",
      "tensor(0.5002, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4679, grad_fn=<SumBackward0>)\n",
      "tensor(0.4679, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4189, grad_fn=<SumBackward0>)\n",
      "tensor(0.4189, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3908, grad_fn=<SumBackward0>)\n",
      "tensor(0.3908, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3992, grad_fn=<SumBackward0>)\n",
      "tensor(0.3992, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4289, grad_fn=<SumBackward0>)\n",
      "tensor(0.4289, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4497, grad_fn=<SumBackward0>)\n",
      "tensor(0.4497, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4429, grad_fn=<SumBackward0>)\n",
      "tensor(0.4429, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4142, grad_fn=<SumBackward0>)\n",
      "tensor(0.4142, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3860, grad_fn=<SumBackward0>)\n",
      "tensor(0.3860, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3766, grad_fn=<SumBackward0>)\n",
      "tensor(0.3766, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3871, grad_fn=<SumBackward0>)\n",
      "tensor(0.3871, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4026, grad_fn=<SumBackward0>)\n",
      "tensor(0.4026, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.4073, grad_fn=<SumBackward0>)\n",
      "tensor(0.4073, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3977, grad_fn=<SumBackward0>)\n",
      "tensor(0.3977, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3835, grad_fn=<SumBackward0>)\n",
      "tensor(0.3835, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3770, grad_fn=<SumBackward0>)\n",
      "tensor(0.3770, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3821, grad_fn=<SumBackward0>)\n",
      "tensor(0.3821, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3918, grad_fn=<SumBackward0>)\n",
      "tensor(0.3918, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3960, grad_fn=<SumBackward0>)\n",
      "tensor(0.3960, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3909, grad_fn=<SumBackward0>)\n",
      "tensor(0.3909, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3815, grad_fn=<SumBackward0>)\n",
      "tensor(0.3815, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3757, grad_fn=<SumBackward0>)\n",
      "tensor(0.3757, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3769, grad_fn=<SumBackward0>)\n",
      "tensor(0.3769, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3816, grad_fn=<SumBackward0>)\n",
      "tensor(0.3816, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3839, grad_fn=<SumBackward0>)\n",
      "tensor(0.3839, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3813, grad_fn=<SumBackward0>)\n",
      "tensor(0.3813, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3766, grad_fn=<SumBackward0>)\n",
      "tensor(0.3766, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3744, grad_fn=<SumBackward0>)\n",
      "tensor(0.3744, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3762, grad_fn=<SumBackward0>)\n",
      "tensor(0.3762, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3794, grad_fn=<SumBackward0>)\n",
      "tensor(0.3794, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3805, grad_fn=<SumBackward0>)\n",
      "tensor(0.3805, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3785, grad_fn=<SumBackward0>)\n",
      "tensor(0.3785, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3757, grad_fn=<SumBackward0>)\n",
      "tensor(0.3757, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3756, grad_fn=<SumBackward0>)\n",
      "tensor(0.3756, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3770, grad_fn=<SumBackward0>)\n",
      "tensor(0.3770, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3768, grad_fn=<SumBackward0>)\n",
      "tensor(0.3768, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3753, grad_fn=<SumBackward0>)\n",
      "tensor(0.3753, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3753, grad_fn=<SumBackward0>)\n",
      "tensor(0.3753, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3760, grad_fn=<SumBackward0>)\n",
      "tensor(0.3760, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3755, grad_fn=<SumBackward0>)\n",
      "tensor(0.3755, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3745, grad_fn=<SumBackward0>)\n",
      "tensor(0.3745, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3750, grad_fn=<SumBackward0>)\n",
      "tensor(0.3750, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3749, grad_fn=<SumBackward0>)\n",
      "tensor(0.3749, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3744, grad_fn=<SumBackward0>)\n",
      "tensor(0.3744, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "tensor(0.3746, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "tensor(0.3743, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "tensor(0.3742, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "tensor(0.3741, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "tensor(0.3740, grad_fn=<SumBackward0>)\n",
      "test loss:  tensor(0.3739, grad_fn=<SumBackward0>)\n",
      "tensor(0.3739, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "featureSet_list = [1, 2]\n",
    "\n",
    "label = torch.tensor([0., 1], requires_grad=True)\n",
    "\n",
    "hidden_layers = list()\n",
    "transferBridge_set = list()\n",
    "\n",
    "for i in range(3):\n",
    "    transferBridge_set.append(StudentMLP(2))\n",
    "    hidden_layers.append(label)\n",
    "\n",
    "    \n",
    "test_common_representation = torch.tensor([0., 1], requires_grad=True)\n",
    "test_optimizer = optim.Adam([test_common_representation], lr=1, betas=(0.9, 0.999))\n",
    "\n",
    "transferBridge_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for converge in range(100):\n",
    "    tmp_loss = torch.zeros((len(featureSet_list)), dtype=torch.float32)\n",
    "    for i in range(len(featureSet_list)):\n",
    "        outputs = transferBridge_set[i](test_common_representation)\n",
    "        tmp_loss[i] = transferBridge_criterion(hidden_layers[i], outputs)\n",
    "    \n",
    "#     test_loss = None\n",
    "\n",
    "    test_loss = torch.sum(tmp_loss)\n",
    "    print(\"test loss: \", test_loss)\n",
    "\n",
    "#     test_loss.backward(retain_graph=True)\n",
    "    test_loss.backward()\n",
    "    test_optimizer.step()\n",
    "    print(test_loss)\n",
    "    test_optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282c183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:FL] *",
   "language": "python",
   "name": "conda-env-FL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
